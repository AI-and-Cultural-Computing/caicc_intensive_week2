{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMf6daULI5Si6WxOvlrlqyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-and-Cultural-Computing/caicc_intensive_week2/blob/main/hello_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we're going to be taking a very brief look at the \"hello world\" of computer vision and deep learning: recognizing single digit numbers from pictures. We're going to assume just a little bit of familiarity with Python but not much, explaining the rational of why code is written the way it is more than we're going to explain the syntax itself. There will be pointers to other references, tutorials, and the like as we go.\n",
        "\n",
        "Why do we start with single digits? Well this is actually a good lesson about the importance of breaking tasks apart. If we tried to have a \"general number recognizer\" we'd have to figure out how to make a network that can parse out numbers of arbitrary lengths, possibly with commas or periods between digits. But! The easier thing to do is to have a network that can see spaces between things in a picture, which is called *segmentation*, and then a separate network that can process an isolated digit. If we can make both of these pieces then we can glue them together in our code in a way that's simpler than making a single network that combines both these tasks together.\n",
        "\n",
        "This tutorial is going to take you through how to make a neural network yourself and train it using a machine learning library called TensorFlow. TensorFlow is one of the major machine learning libraries, along with PyTorch---which is the library that the Deep Learning for Coders book is built off of---and in my opinion TensorFlow is the easier library to get started with if you just want to make a simple neural network quickly. TensorFlow also has the advantage that it can run on even tiny low-powered machines or in the browser, via the Tensorflow-lite and TensorFlow.js libraries respectively. One last note, the code in this tutorial is inspired by the official [tensorflow tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner), with some simplifications where I think it's helpful. The explanatory text, on the other hand, is original to this document. \n",
        "\n",
        "So the very first thing we need to do is to load the Tensorflow library, since that's what we're going to use for this little introduction. You don't even need to install anything because tensorflow, like colab, is a google product so they bundle it in. Convenient yet possibly problematic if you're old enough to remember when Microsoft got in hot water for bundling a web browser with an operating sytem. "
      ],
      "metadata": {
        "id": "luUMKDwWiV90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NOzXbwm7iOnk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing is to load our dataset. We're taking a shortcut here and using the famous [MNIST digit training set](https://en.wikipedia.org/wiki/MNIST_database), that's a bunch of small images of digits that are only 28 pixels by 28 pixels each. This dataset is small enough in size and famous enough that it tends to be included inside most machine learning systems, which is true for tensorflow as well."
      ],
      "metadata": {
        "id": "iy_f9s_otA92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(image_train, number_train), (image_test, number_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "gcG5MMRivtQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a18300-d973-4e43-e959-abd718bafeee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we've taken the data from the `mnist` dataset and loaded it into four separate variables. \n",
        " \n",
        " - `image_train` : the list of actual data of the images we'll use for training\n",
        " - `number_train` : the list of (correct) classifications of the images for training\n",
        " - `image_test` :  the list of data for the images we'll use to test our trained algorithm\n",
        " - `number_test` : the list of classifications of the images for testing\n",
        "\n",
        " we can take a look at how this data is formatted easily enough\n",
        "\n",
        " since this is a \"list\" (actually an \"array\" as provided by the numpy library, which you can think of like a list but it's more efficient for the computer to use) we can access the first element and see what it looks like"
      ],
      "metadata": {
        "id": "edxT_ot0vyR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lrAsdt_1nsB",
        "outputId": "571276ca-067f-49b2-d696-b8f6d3e1d6de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's a two-dimensional structure of numbers (28 lists of 28 elements) from 0 to 255. You can't see all the entries but you can calculate out that if it's 28x28 there must be a total of 784 entries in the whole image. \n",
        "\n",
        "This makes sense because a black and white image, like these are, has a single number per pixel ranging from 0-255 with 0 representing black and 255 representing white \n",
        "\n",
        "here, let's take a look at what exactly this number is"
      ],
      "metadata": {
        "id": "m9_otzHi3Q-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5EmvKyE5Rrp",
        "outputId": "0230e6cf-6bc8-463b-a3f3-aadb4b513dd5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's kind of hard to look at the image and tell what it is\n",
        "\n",
        "here, if we write just a little bit of python code we can kinda visualize this a little bit better"
      ],
      "metadata": {
        "id": "02babBwx5cD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showImg(img):\n",
        "  for i in range(0,27):\n",
        "    print(\"\")\n",
        "    for j in range(0,27):\n",
        "      if img[i][j] > 0:\n",
        "        print(\"w\", end=\" \")\n",
        "      else:\n",
        "        print(\"b\", end=\" \")\n",
        "\n",
        "showImg(image_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6jbU3eW5yWp",
        "outputId": "cfcc76e1-6de9-499d-d1aa-b713842bf7a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w w w w w w w b b b \n",
            "b b b b b b b b w w w w w w w w w w w w w w w w b b b \n",
            "b b b b b b b w w w w w w w w w w w w w w w w b b b b \n",
            "b b b b b b b w w w w w w w w w w w b b b b b b b b b \n",
            "b b b b b b b b w w w w w w w b w w b b b b b b b b b \n",
            "b b b b b b b b b w w w w w b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b w w w w b b b b b b b b b b b b \n",
            "b b b b b b b b b b b w w w w b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w b b b b b b b b b \n",
            "b b b b b b b b b b b b b w w w w w w b b b b b b b b \n",
            "b b b b b b b b b b b b b b w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b b b b b w w w w w b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b w w w w b b b b b b \n",
            "b b b b b b b b b b b b b b w w w w w w w b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b w w w w w w w w w b b b b b b b b \n",
            "b b b b b b b b w w w w w w w w w w b b b b b b b b b \n",
            "b b b b b b w w w w w w w w w w b b b b b b b b b b b \n",
            "b b b b w w w w w w w w w w b b b b b b b b b b b b b \n",
            "b b b b w w w w w w w w b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there, now it's a little easier to visualize! It's sort of a scraggly five!\n",
        "\n",
        "For machine learning, though, we generally want to *normalize* our data between 0 and 1 as much as possible. To do that we can do kind of a neat trick and just divide the entire two dimensional image by 255.0."
      ],
      "metadata": {
        "id": "NljPYV7G8KZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_train_normal = image_train / 255.0\n",
        "image_test_normal = image_test / 255.0"
      ],
      "metadata": {
        "id": "q5cOmTm7-Gcr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our data we need to actually build our neural network. We're going to do the simplest possible we can. First, though, let's think about what the restrictions we're dealing with really even are: we know that there must be 784 inputs---one for each pixel---and we know there must be 10 outputs, one representing each of the numbers 0 through 9. \n",
        "\n",
        "But this is a \"deep\" network, meaning that to work it also needs to have some kind of \"hidden\" layers between the inputs and outputs. \n",
        "\n",
        "We're going to start with just one hidden layer and we're going to make it 32 nodes in size. Why? Honestly it's kind of arbitrary! If you have too big of hidden layers or too many, you risk \"overfitting\", meaning that your network has just \"memorized\" the training data and will perform terribly when run on perfectly valid data that's too different from the training set. Too few and there's not enough flexibility to actually do any generalization.\n",
        "\n",
        "Now, if you're thinking to yourself that there *should* be some kind of definitive answer I don't blame you, but deep learning is still very \"ad hoc\", meaning that we try things and figure out strategies on a case by case basis rather than having rigorous answers.\n",
        "\n",
        "Despite all its potential it's not yet a science or an engineering discipline. Yet.\n",
        "\n",
        "So to make a network is going to be pretty easy and I'll show you the code first and then explain it bit by bit. "
      ],
      "metadata": {
        "id": "LznRM548-QRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(32, activation='sigmoid'),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "DsAqcRPKCmhH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that we're defining our model as taking 784 inputs---telling it to expect the 784 inputs as a 28x28 list of lists---then we fully connect every one of those inputs to every one of the nodes in the middle layer, that's what `Dense` means. Already we can do some math and figure out how many weights just are connecting the input to the middle layer: 784 * 32 = 25088 weights. The line `activation='sigmoid]` is the activation we've talked about before, the classic [logistic function](https://en.wikipedia.org/wiki/Logistic_function), and so this function is applied as data exits each node. \n",
        "\n",
        "From there we can connect the hidden layer to the output layer by adding another \"dense\" layer with ten nodes, adding another 320 weights to be adjusted.\n",
        "\n",
        "So how do we read the outputs? Well, there's ten outputs, one for each possible digit. The numbers the network is trained to calculate for each output are the odds that that label is correct. You can interpret the biggest of all the output numbers as the network's \"best guess\" for what the correct answer is. I put \"guess\" in quotes because there's no thinking or real guesswork involved, it's just a calculated estimate: input to output, deterministically. But as long as we're internally careful to remember that there's no thinking, \"guess\" is a pretty clear and evocative word.\n",
        "\n",
        "We can actually retrieve a prediction like this:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ynt0cY1EC_bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(image_train_normal[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTVs7ihAg5j1",
        "outputId": "1152eb95-3d34-470f-a76d-7616651ddce9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[-0.04010337,  0.3045755 ,  0.1621109 , -1.2246321 , -0.320374  ,\n",
              "        -0.5838765 , -0.44233322, -0.23140463,  1.6447451 ,  0.01815933]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, a model is just a function that takes in inputs and gives out a list of lists of outputs.\n",
        "\n",
        "Okay, but what, how on earth did we run the model if we haven't *trained* it yet? \n",
        "\n",
        "Well the secret is that every, and I do mean every, machine learning system initializes the weights to *something*. Generally they're always set to small randomly assigned numbers, to provide the variation needed for the learning algorithm to be most effective.\n",
        "\n",
        "Now you'll notice that these numbers are both positive and negative and yet they're somehow related the \"guess\" the model is making. These are called [logits](https://en.wikipedia.org/wiki/Logit) in the machine learning literature.\n",
        "\n",
        "We can interpret this prediction of logits with something called the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function, which takes these positive and negative numbers and then converts them into properly normalized probabilities.\n",
        "\n",
        "You don't have to write the softmax function, thankfully, it already exists in any machine learning library: specifically in Tensorflow, which we're using in this tutorial, it's `tf.nn.softmax`.\n",
        "\n",
        "So we can try this use of `model` again with:"
      ],
      "metadata": {
        "id": "kE8TkfDJhmSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(model(image_train_normal[:1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RwPrISCLiWc",
        "outputId": "9de7957e-8f13-4c90-eb18-029df99568db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.07562005, 0.10674044, 0.09256728, 0.02313144, 0.05713693,\n",
              "        0.04390149, 0.05057673, 0.06245336, 0.4077156 , 0.08015674]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so now we can at least read these like percentages but it would be nice to make it more readable still. We're going to do that with a Python function that goes through the entire list of results and finds the probability that's the largest and, then, returns both the number and the probability that the number is correct.\n",
        "\n",
        "Exercise: try to step through the logic of the for-loop in the function in order to understand why it's written the way it is"
      ],
      "metadata": {
        "id": "M6hc38xUNcqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bestGuess(ls):\n",
        "  ps = tf.nn.softmax(ls)\n",
        "  maxp = 0\n",
        "  maxi = -1\n",
        "  for i in range(0,tf.size(ps)):\n",
        "    p = ps[i]\n",
        "    if p > maxp:\n",
        "      maxp = p\n",
        "      maxi = i\n",
        "  return (maxi, maxp)\n",
        "\n",
        "bestGuess(model(image_train[:1])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzPUENoHQa3A",
        "outputId": "ba8995a8-c9ec-4c15-8cc1-e4fade2992e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, <tf.Tensor: shape=(), dtype=float32, numpy=0.43388438>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we're all set to actually train the model. The last thing we need to do is figure out what it means for a prediction to be right or wrong. In other words, to define the *loss* function.\n",
        "\n",
        "If the output of our model is to calculate numbers-as-values we'd use the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error), but since we're calculating probabilities we want to use something called the, and bear with me here, [categorical cross entropy loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/). So that's a dense term but all it really means is that it's a way to quantify how \"wrong\" a guess at categorizing something is.\n",
        "\n",
        "To prepare our model for training we just need to do one last thing: compile it.\n",
        "\n",
        "This turns our description of our model and our loss into a thing we can train."
      ],
      "metadata": {
        "id": "X0emNuvDWRTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "NdyVWSeEcBg1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(image_train_normal, number_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx1qBQKOeStD",
        "outputId": "de43c9c9-7908-4c05-f898-7eb8b573825e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5558\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2580\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2148\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1898\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1732\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0e62132110>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, you've now trained a model!\n",
        "\n",
        "Let's test it now on elements of our training set. \n",
        "\n",
        "We're going write just a little more Python glue code---that is, code that ties together code from libraries---to print out a comparison of guesses with numbers!"
      ],
      "metadata": {
        "id": "Asjon-_VfYyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(f'Our prediction for image {i} is {bestGuess(model(image_test_normal[i:i+1])[0])}')\n",
        "  print(f'The real number is {number_test[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xckJiC1ofuzn",
        "outputId": "bede4a85-740c-47fe-8042-85539f26d16b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our prediction for image 0 is (7, <tf.Tensor: shape=(), dtype=float32, numpy=0.9976562>)\n",
            "The real number is 7\n",
            "Our prediction for image 1 is (2, <tf.Tensor: shape=(), dtype=float32, numpy=0.9747999>)\n",
            "The real number is 2\n",
            "Our prediction for image 2 is (1, <tf.Tensor: shape=(), dtype=float32, numpy=0.9962482>)\n",
            "The real number is 1\n",
            "Our prediction for image 3 is (0, <tf.Tensor: shape=(), dtype=float32, numpy=0.9982387>)\n",
            "The real number is 0\n",
            "Our prediction for image 4 is (4, <tf.Tensor: shape=(), dtype=float32, numpy=0.9917726>)\n",
            "The real number is 4\n",
            "Our prediction for image 5 is (1, <tf.Tensor: shape=(), dtype=float32, numpy=0.99469274>)\n",
            "The real number is 1\n",
            "Our prediction for image 6 is (4, <tf.Tensor: shape=(), dtype=float32, numpy=0.98481303>)\n",
            "The real number is 4\n",
            "Our prediction for image 7 is (9, <tf.Tensor: shape=(), dtype=float32, numpy=0.9920874>)\n",
            "The real number is 9\n",
            "Our prediction for image 8 is (5, <tf.Tensor: shape=(), dtype=float32, numpy=0.56872857>)\n",
            "The real number is 5\n",
            "Our prediction for image 9 is (9, <tf.Tensor: shape=(), dtype=float32, numpy=0.9804971>)\n",
            "The real number is 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad for being so simple! If you look at the probabilities you'll probably see a mixture of ones in the 90s and some much lower! This tells us that the model is generally very confident but there are ambiguous cases it has trouble with.\n",
        "\n",
        "Try visualizing one of the lower confidence predictions with our `showImg` function from above"
      ],
      "metadata": {
        "id": "c2rnwTFCidCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad_img = 8 #change this to the number of the image you want to visualize\n",
        "showImg(image_test_normal[bad_img])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q2YxIeRkhE9",
        "outputId": "ab171270-6486-41cd-be92-46b09e6379ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b w w w w w w w w b b \n",
            "b b b b b b b b b b b b b b b w w w w w w w w w w b b \n",
            "b b b b b b b b b b b b w w w w w w w w w w w w w b b \n",
            "b b b b b b b b b b b b w w w w w w w w w w w w w b b \n",
            "b b b b b b b b w w w b w w w w w w w w w b b b b b b \n",
            "b b b b b b b w w w w b b w b w w b b b b b b b b b b \n",
            "b b b b b b w w w w w b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w w b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w b b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w w b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w w w w w w w w w w b b b b b b b b b \n",
            "b b b b b w w w w w w w w w w w w w w b b b b b b b b \n",
            "b b b b b w w w w w w w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b w w w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w b b b b b b b b b \n",
            "b b b b b b b b b b b b b b w w w b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if this was all there was to computer vision that would be convenient, but there's actually a lot more to it! This is a really simple black and white photo problem with very few pixels all told trying to identify one of ten very different looking objects, always centered in the image in roughly the same spot, simply because there's so little wiggle room in number of pixels.\n",
        "\n",
        "That last complication is the first one I want to talk about. So the neural network here has \"learned\" the general shape of different digits but has tied them to particular *locations* in the image. The neural network is, as we've established, just an equation. A really really big equation but it's still just an equation. You and I can look at a picture of a 5 moved up and down, left or right, and still see it's a 5. The neural network has no way to understand that a 5 over on the right hand side is inherently the same thing as a 5 on the left hand side. You *could* correct this just by including enough of examples in the training but that's going to become unwieldy, maybe not even possible, given your data set. You're often just working with the data you have, not the data you *want*!\n",
        "\n",
        "Instead, though, there's a way to try and desensitize image recognition models from small variations in shape and positioning: [*convolutional* neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network). Until recently CNNs were basically the gold standard for a lot of computer vision projects. The basic idea is that they add an extra layer that lets the model make its calculations not on a pixel-by-pixel basis but by considering small neighborhoods around each pixel. Even making the neighborhood just a few pixels in radius can make a drastic difference in how well the model performs! But some of the really successful convolutional models actually use multiple layers of neighborhoods and clever tricks to make them work. One of these is [resnet50](https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33), which if you go on to read the Deep Learning for Coders text is used in some of their first examples of how to quickly build an image recognition system. \n",
        "\n",
        "Building a CNN in most modern frameworks really isn't that much harder than what we already did. As an exercise, see if you can add a convolutional layer by looking at the Tensorflow documentation for the [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?hl=en) layer type.\n",
        "\n",
        "Now the really cutting edge stuff in computer vision, though, involves a variant of something you might have seen in the natural language processing world: *transformers*. The same way NLP transformers allow for correlative relationships between parts of a sentence or paragraph, vision transformers allow for more complicated relationships across the image not just in small neighborhoods!\n",
        "\n",
        "If you want to get a preview of how to use vision transformers you can start with this [blog post](https://huggingface.co/blog/vision-transformers) on the hugging face site."
      ],
      "metadata": {
        "id": "rj0gpDwynhC8"
      }
    }
  ]
}