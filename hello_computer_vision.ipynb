{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQXZV8HMR6btmrXKGVHeYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-and-Cultural-Computing/caicc_intensive_week2/blob/main/hello_computer_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we're going to be taking a very brief look at the \"hello world\" of computer vision and deep learning: recognizing single digit numbers from pictures. We're going to assume just a little bit of familiarity with Python but not much, explaining the rational of why code is written the way it is more than we're going to explain the syntax itself. There will be pointers to other references, tutorials, and the like as we go.\n",
        "\n",
        "Why do we start with single digits? Well this is actually a good lesson about the importance of breaking tasks apart. If we tried to have a \"general number recognizer\" we'd have to figure out how to make a network that can parse out numbers of arbitrary lengths, possibly with commas or periods between digits. But! The easier thing to do is to have a network that can see spaces between things in a picture, which is called *segmentation*, and then a separate network that can process an isolated digit. If we can make both of these pieces then we can glue them together in our code in a way that's simpler than making a single network that combines both these tasks together.\n",
        "\n",
        "One last note, the code in this tutorial is inspired by the official [tensorflow tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner), with some simplifications where I think it's helpful.\n",
        "\n",
        "So the very first thing we need to do is to load the Tensorflow library, since that's what we're going to use for this little introduction. You don't even need to install anything because tensorflow, like colab, is a google product so they bundle it in. Convenient yet possibly problematic if you're old enough to remember when Microsoft got in hot water for bundling a web browser with an operating sytem. "
      ],
      "metadata": {
        "id": "luUMKDwWiV90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NOzXbwm7iOnk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing is to load our dataset. We're taking a shortcut here and using the famous [MNIST digit training set](https://en.wikipedia.org/wiki/MNIST_database), that's a bunch of small images of digits that are only 28 pixels by 28 pixels each. This dataset is small enough in size and famous enough that it tends to be included inside most machine learning systems, which is true for tensorflow as well."
      ],
      "metadata": {
        "id": "iy_f9s_otA92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(image_train, number_train), (image_test, number_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "gcG5MMRivtQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abbdf59-a71f-48f1-8f8d-c09762ea267c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we've taken the data from the `mnist` dataset and loaded it into four separate variables. \n",
        " \n",
        " - `image_train` : the list of actual data of the images we'll use for training\n",
        " - `number_train` : the list of (correct) classifications of the images for training\n",
        " - `image_test` :  the list of data for the images we'll use to test our trained algorithm\n",
        " - `number_test` : the list of classifications of the images for testing\n",
        "\n",
        " we can take a look at how this data is formatted easily enough\n",
        "\n",
        " since this is a \"list\" (actually an \"array\" as provided by the numpy library, which you can think of like a list but it's more efficient for the computer to use) we can access the first element and see what it looks like"
      ],
      "metadata": {
        "id": "edxT_ot0vyR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lrAsdt_1nsB",
        "outputId": "773de361-7a19-4918-ce52-aaac24dab1c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
              "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
              "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
              "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
              "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
              "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
              "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
              "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
              "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
              "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
              "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
              "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
              "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
              "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
              "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
              "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
              "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
              "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's a two-dimensional structure of numbers (28 lists of 28 elements) from 0 to 255. You can't see all the entries but you can calculate out that if it's 28x28 there must be a total of 784 entries in the whole image. \n",
        "\n",
        "This makes sense because a black and white image, like these are, has a single number per pixel ranging from 0-255 with 0 representing black and 255 representing white \n",
        "\n",
        "here, let's take a look at what exactly this number is"
      ],
      "metadata": {
        "id": "m9_otzHi3Q-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5EmvKyE5Rrp",
        "outputId": "33b1af88-1194-4344-f36b-9066d7ae04f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's kind of hard to look at the image and tell what it is\n",
        "\n",
        "here, if we write just a little bit of python code we can kinda visualize this a little bit better"
      ],
      "metadata": {
        "id": "02babBwx5cD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showImg(img):\n",
        "  for i in range(0,27):\n",
        "    print(\"\")\n",
        "    for j in range(0,27):\n",
        "      if img[i][j] > 0:\n",
        "        print(\"w\", end=\" \")\n",
        "      else:\n",
        "        print(\"b\", end=\" \")\n",
        "\n",
        "showImg(image_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6jbU3eW5yWp",
        "outputId": "fac9f16b-df24-4ed3-bbba-c6f8652a1949"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w w w w w w w b b b \n",
            "b b b b b b b b w w w w w w w w w w w w w w w w b b b \n",
            "b b b b b b b w w w w w w w w w w w w w w w w b b b b \n",
            "b b b b b b b w w w w w w w w w w w b b b b b b b b b \n",
            "b b b b b b b b w w w w w w w b w w b b b b b b b b b \n",
            "b b b b b b b b b w w w w w b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b w w w w b b b b b b b b b b b b \n",
            "b b b b b b b b b b b w w w w b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w b b b b b b b b b \n",
            "b b b b b b b b b b b b b w w w w w w b b b b b b b b \n",
            "b b b b b b b b b b b b b b w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b b b b b w w w w w b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b w w w w b b b b b b \n",
            "b b b b b b b b b b b b b b w w w w w w w b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b w w w w w w w w w b b b b b b b b \n",
            "b b b b b b b b w w w w w w w w w w b b b b b b b b b \n",
            "b b b b b b w w w w w w w w w w b b b b b b b b b b b \n",
            "b b b b w w w w w w w w w w b b b b b b b b b b b b b \n",
            "b b b b w w w w w w w w b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there, now it's a little easier to visualize! It's sort of a scraggly five!\n",
        "\n",
        "For machine learning, though, we generally want to *normalize* our data between 0 and 1 as much as possible. To do that we can do kind of a neat trick and just divide the entire two dimensional image by 255.0."
      ],
      "metadata": {
        "id": "NljPYV7G8KZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_train = image_train / 255.0\n",
        "image_test = image_test / 255.0"
      ],
      "metadata": {
        "id": "q5cOmTm7-Gcr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our data we need to actually build our neural network. We're going to do the simplest possible we can. First, though, let's think about what the restrictions we're dealing with really even are: we know that there must be 784 inputs---one for each pixel---and we know there must be 10 outputs, one representing each of the numbers 0 through 9. \n",
        "\n",
        "But this is a \"deep\" network, meaning that to work it also needs to have some kind of \"hidden\" layers between the inputs and outputs. \n",
        "\n",
        "We're going to start with just one hidden layer and we're going to make it 32 nodes in size. Why? Honestly it's kind of arbitrary! If you have too big of hidden layers or too many, you risk \"overfitting\", meaning that your network has just \"memorized\" the training data and will perform terribly when run on perfectly valid data that's too different from the training set. Too few and there's not enough flexibility to actually do any generalization.\n",
        "\n",
        "Now, if you're thinking to yourself that there *should* be some kind of definitive answer I don't blame you, but deep learning is still very \"ad hoc\", meaning that we try things and figure out strategies on a case by case basis rather than having rigorous answers.\n",
        "\n",
        "Despite all its potential it's not yet a science or an engineering discipline. Yet.\n",
        "\n",
        "So to make a network is going to be pretty easy and I'll show you the code first and then explain it bit by bit. "
      ],
      "metadata": {
        "id": "LznRM548-QRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(32, activation='sigmoid'),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "DsAqcRPKCmhH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that we're defining our model as taking 784 inputs---telling it to expect the 784 inputs as a 28x28 list of lists---then we fully connect every one of those inputs to every one of the nodes in the middle layer, that's what `Dense` means. Already we can do some math and figure out how many weights just are connecting the input to the middle layer: 784 * 32 = 25088 weights. The line `activation='sigmoid]` is the activation we've talked about before, the classic [logistic function](https://en.wikipedia.org/wiki/Logistic_function), and so this function is applied as data exits each node. \n",
        "\n",
        "From there we can connect the hidden layer to the output layer by adding another \"dense\" layer with ten nodes, adding another 320 weights to be adjusted.\n",
        "\n",
        "So how do we read the outputs? Well, there's ten outputs, one for each possible digit. The numbers the network is trained to calculate for each output are the odds that that label is correct. You can interpret the biggest of all the output numbers as the network's \"best guess\" for what the correct answer is. I put \"guess\" in quotes because there's no thinking or real guesswork involved, it's just a calculated estimate: input to output, deterministically. But as long as we're internally careful to remember that there's no thinking, \"guess\" is a pretty clear and evocative word.\n",
        "\n",
        "We can actually retrieve a prediction like this:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ynt0cY1EC_bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(image_train[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTVs7ihAg5j1",
        "outputId": "c54d0520-5380-4f60-87a2-ba3a873dc027"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[ 0.23398848,  1.1608454 ,  0.59939647, -0.950471  , -0.43713567,\n",
              "        -0.22372867,  0.03539851,  0.0704702 , -0.1868309 ,  0.18669273]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, a model is just a function that takes in inputs and gives out a list of lists of outputs.\n",
        "\n",
        "Okay, but what, how on earth did we run the model if we haven't *trained* it yet? \n",
        "\n",
        "Well the secret is that every, and I do mean every, machine learning system initializes the weights to *something*. Generally they're always set to small randomly assigned numbers, to provide the variation needed for the learning algorithm to be most effective.\n",
        "\n",
        "Now you'll notice that these numbers are both positive and negative and yet they're somehow related the \"guess\" the model is making. These are called [logits](https://en.wikipedia.org/wiki/Logit) in the machine learning literature.\n",
        "\n",
        "We can interpret this prediction of logits with something called the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function, which takes these positive and negative numbers and then converts them into properly normalized probabilities.\n",
        "\n",
        "You don't have to write the softmax function, thankfully, it already exists in any machine learning library: specifically in Tensorflow, which we're using in this tutorial, it's `tf.nn.softmax`.\n",
        "\n",
        "So we can try this use of `model` again with:"
      ],
      "metadata": {
        "id": "kE8TkfDJhmSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(model(image_train[:1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RwPrISCLiWc",
        "outputId": "f3064000-bdc0-42e5-bd5e-8db1969ab61e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.10312701, 0.26055613, 0.14861652, 0.03154774, 0.05271169,\n",
              "        0.06525119, 0.0845524 , 0.08757041, 0.06770378, 0.09836309]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so now we can at least read these like percentages but it would be nice to make it more readable still. We're going to do that with a Python function that goes through the entire list of results and finds the probability that's the largest and, then, returns both the number and the probability that the number is correct.\n",
        "\n",
        "Exercise: try to step through the logic of the for-loop in the function in order to understand why it's written the way it is"
      ],
      "metadata": {
        "id": "M6hc38xUNcqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bestGuess(ls):\n",
        "  ps = tf.nn.softmax(ls)\n",
        "  maxp = 0\n",
        "  maxi = -1\n",
        "  for i in range(0,tf.size(ps)):\n",
        "    p = ps[i]\n",
        "    if p > maxp:\n",
        "      maxp = p\n",
        "      maxi = i\n",
        "  return (maxi, maxp)\n",
        "\n",
        "bestGuess(model(image_train[:1])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzPUENoHQa3A",
        "outputId": "ca3e82cd-d9f3-4925-92cf-32e5a2c2e1cc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, <tf.Tensor: shape=(), dtype=float32, numpy=0.9002729>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we're all set to actually train the model. The last thing we need to do is figure out what it means for a prediction to be right or wrong. In other words, to define the *loss* function.\n",
        "\n",
        "If the output of our model is to calculate numbers-as-values we'd use the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error), but since we're calculating probabilities we want to use something called the, and bear with me here, [categorical cross entropy loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/). So that's a dense term but all it really means is that it's a way to quantify how \"wrong\" a guess at categorizing something is.\n",
        "\n",
        "To prepare our model for training we just need to do one last thing: compile it.\n",
        "\n",
        "This turns our description of our model and our loss into a thing we can train."
      ],
      "metadata": {
        "id": "X0emNuvDWRTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "NdyVWSeEcBg1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(image_train, number_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx1qBQKOeStD",
        "outputId": "2b512324-5c22-4dd1-a645-9782b709154f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5378\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2547\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2122\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1872\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1703\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74c10ff550>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, you've now trained a model!\n",
        "\n",
        "Let's test it now on elements of our training set. \n",
        "\n",
        "We're going write just a little more Python glue code---that is, code that ties together code from libraries---to print out a comparison of guesses with numbers!"
      ],
      "metadata": {
        "id": "Asjon-_VfYyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(f'Our prediction for image {i} is {bestGuess(model(image_test[i:i+1])[0])}')\n",
        "  print(f'The real number is {number_test[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xckJiC1ofuzn",
        "outputId": "22d5a447-ae59-40d9-aa4f-83ffe1482ce0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our prediction for image 0 is (7, <tf.Tensor: shape=(), dtype=float32, numpy=0.99907875>)\n",
            "The real number is 7\n",
            "Our prediction for image 1 is (2, <tf.Tensor: shape=(), dtype=float32, numpy=0.98654515>)\n",
            "The real number is 2\n",
            "Our prediction for image 2 is (1, <tf.Tensor: shape=(), dtype=float32, numpy=0.992543>)\n",
            "The real number is 1\n",
            "Our prediction for image 3 is (0, <tf.Tensor: shape=(), dtype=float32, numpy=0.9990615>)\n",
            "The real number is 0\n",
            "Our prediction for image 4 is (4, <tf.Tensor: shape=(), dtype=float32, numpy=0.9872649>)\n",
            "The real number is 4\n",
            "Our prediction for image 5 is (1, <tf.Tensor: shape=(), dtype=float32, numpy=0.9944832>)\n",
            "The real number is 1\n",
            "Our prediction for image 6 is (4, <tf.Tensor: shape=(), dtype=float32, numpy=0.9708294>)\n",
            "The real number is 4\n",
            "Our prediction for image 7 is (9, <tf.Tensor: shape=(), dtype=float32, numpy=0.9796934>)\n",
            "The real number is 9\n",
            "Our prediction for image 8 is (5, <tf.Tensor: shape=(), dtype=float32, numpy=0.66218424>)\n",
            "The real number is 5\n",
            "Our prediction for image 9 is (9, <tf.Tensor: shape=(), dtype=float32, numpy=0.98055947>)\n",
            "The real number is 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad for being so simple! If you look at the probabilities you'll probably see a mixture of ones in the 90s and some much lower! This tells us that the model is generally very confident but there are ambiguous cases it has trouble with.\n",
        "\n",
        "Try visualizing one of the lower confidence predictions with our `showImg` function from above"
      ],
      "metadata": {
        "id": "c2rnwTFCidCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad_img = 8 #change this to the number of the image you want to visualize\n",
        "showImg(image_test[bad_img])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q2YxIeRkhE9",
        "outputId": "5e81275e-8dd9-488d-bdc0-57360c7ff4af"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b w w w w w w w w b b \n",
            "b b b b b b b b b b b b b b b w w w w w w w w w w b b \n",
            "b b b b b b b b b b b b w w w w w w w w w w w w w b b \n",
            "b b b b b b b b b b b b w w w w w w w w w w w w w b b \n",
            "b b b b b b b b w w w b w w w w w w w w w b b b b b b \n",
            "b b b b b b b w w w w b b w b w w b b b b b b b b b b \n",
            "b b b b b b w w w w w b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w w b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w b b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w w b b b b b b b b b b b b b b b b b \n",
            "b b b b b w w w w w w w w w w w w w b b b b b b b b b \n",
            "b b b b b w w w w w w w w w w w w w w b b b b b b b b \n",
            "b b b b b w w w w w w w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b w w w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b w w w w w w w w w b b b b b b b \n",
            "b b b b b b b b b b b b w w w w w w b b b b b b b b b \n",
            "b b b b b b b b b b b b b b w w w b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b \n",
            "b b b b b b b b b b b b b b b b b b b b b b b b b b b "
          ]
        }
      ]
    }
  ]
}